{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e287d48-2ee0-4284-8c9b-382215f1ef37",
   "metadata": {},
   "source": [
    "# ADAML, machine learning: workshop 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763bc067-c619-495c-88fc-9094076a883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\niemelav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3eabb9-5b18-40df-b072-2c303fae0108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK resources\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95faa4b8-c638-407a-b8a3-8acd2a983e50",
   "metadata": {},
   "source": [
    "# 0. Data importing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6579cd19-dc91-4678-ab55-e6de01a761c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Label\n",
      "0  Budget to set scene for election\\n \\n Gordon B...      0\n",
      "1  Army chiefs in regiments decision\\n \\n Militar...      0\n",
      "2  Howard denies split over ID cards\\n \\n Michael...      0\n",
      "3  Observers to monitor UK election\\n \\n Minister...      0\n",
      "4  Kilroy names election seat target\\n \\n Ex-chat...      0\n",
      "Text     0\n",
      "Label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"df_file.csv\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.isnull().sum()) # checking whether data contains any nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819004c7-7fee-4c5b-9676-512fd64bc8ac",
   "metadata": {},
   "source": [
    "No nulls!\n",
    "\n",
    "Next: shuffling the data, since the classes are ordered in the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749597ab-035a-4f5d-bee7-de618ec8ac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deutsche Boerse boosts dividend\\n \\n Deutsche ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sayeed to stand down as Tory MP\\n \\n Tory MP J...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Best person' for top legal job\\n \\n The \"best...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US in EU tariff chaos trade row\\n \\n The US ha...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top stars join US tsunami TV show\\n \\n Brad Pi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  Deutsche Boerse boosts dividend\\n \\n Deutsche ...      4\n",
       "1  Sayeed to stand down as Tory MP\\n \\n Tory MP J...      0\n",
       "2  'Best person' for top legal job\\n \\n The \"best...      0\n",
       "3  US in EU tariff chaos trade row\\n \\n The US ha...      4\n",
       "4  Top stars join US tsunami TV show\\n \\n Brad Pi...      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e587e1-f943-4606-8b03-9be9201e33fe",
   "metadata": {},
   "source": [
    "Next: checking duplicates and dropping them if existing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38caee48-d0c4-4e98-aa58-4ae0b1cdedb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "0\n",
      "Text     0\n",
      "Label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c7209-3f91-4088-ba96-369be20b6ca1",
   "metadata": {},
   "source": [
    "98 dublicates dropped\n",
    "\n",
    "Next: defining a function which preprocesses the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3658768-babe-4869-93c5-aaeb20129642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() # lowering the text\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # removes numbers, punctuations, uppercase letters and special symbols\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # collapses multiple spaces/tabs/newlines into one space and removes any spaces at the start or end\n",
    "    words = text.split() # splitting text into words\n",
    "\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] # lemmatization\n",
    "\n",
    "    words = [word for word in words if word not in stop_words] # stop-word removal\n",
    "\n",
    "    return \" \".join(words) # joining words and adding space between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48aeb4a1-92d2-4880-9db7-7f774e27958c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of cleaned text:\n",
      "deutsche boerse boost dividend deutsche boerse german stock exchange trying buy london rival ha said boost dividend payment analyst said move aimed winning investor opposed bid london stock exchange critic takeover complained money could better used returning cash shareholder deutsche boerse also said profit three month december wa euro sale climbed euro lifting revenue year record bn euro frankfurtbased deutsche boerse ha offered bn bn bn euro london stock exchange rival paneuropean bourse euronext working also bid late monday deutsche boerse said would lift dividend payment euro cent euro cent year earlier whiff sweetener anais faraj analyst nomura told bbcs world business report disgruntled shareholder deutsche boerse complaining money used bid could better placed hand paid dividend mr faraj continued deutsche boerse trying buy sense said\n"
     ]
    }
   ],
   "source": [
    "df['new_text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "print(\"\\nExample of cleaned text:\")\n",
    "print(df['new_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a91dfc-85ae-4549-a82a-a79eaeadde65",
   "metadata": {},
   "source": [
    "# 1. Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e035c5-9d38-4b9b-8249-52fbe42b1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"new_text\"]\n",
    "y = df[\"Label\"]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3) # test size = 20% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665bf95-46e3-43e9-bd1a-371fdf74b847",
   "metadata": {},
   "source": [
    "# 3. Vectorization\n",
    "\n",
    "Mapping words into integer IDs based on the frequency and vectorizing text to SEQUENCY_LENGTH length vectors of words integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee74e19b-a6e2-4700-8003-2175d401582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Keras sequences (Training): (1701, 150)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for Keras Vectorization\n",
    "MAX_TOKENS = 65553 # = how many unique words to keep\n",
    "SEQUENCE_LENGTH = 150 # longer texts are truncated and shorter texts are padded\n",
    "\n",
    "# Initialize TextVectorization layer\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\", # outputs integer indices for words\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# Adapt the vectorizer to the training data\n",
    "vectorizer.adapt(X_train) # building a mapping from words to integer IDs based on frequency\n",
    "\n",
    "# Convert text data to sequences of integers\n",
    "X_train_seq = vectorizer(X_train)\n",
    "X_test_seq = vectorizer(X_test)\n",
    "\n",
    "print(f\"Shape of Keras sequences (Training): {X_train_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe771fc3-047e-4cfd-aa82-d62422b4111b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1701, 150), dtype=int64, numpy=\n",
       "array([[ 2893,  1893,   517, ...,     0,     0,     0],\n",
       "       [ 9487,    57,  7931, ...,     0,     0,     0],\n",
       "       [ 4068,  9801,   218, ...,  8002,  2017,     2],\n",
       "       ...,\n",
       "       [  377,    18,   297, ...,     0,     0,     0],\n",
       "       [  292,   114, 15747, ...,   765,    54,   288],\n",
       "       [  703,  4216,   348, ...,   575,    96,  1908]], shape=(1701, 150))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e9109-1e21-4314-a551-4753dd9a8af3",
   "metadata": {},
   "source": [
    "# 4. Embedding and model training\n",
    "\n",
    "Mapping each token ID (= word ID) to a dense vector of size 128. The model used is vanilla RNN (= each hidden state is calculated with tanh-function) with 64 hidden units. The activation function for the output layer used is softmax. For the evaluation accuracy score is used (= how many correct labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2703c45-7f89-4ca4-9e8e-71bb14d54583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niemelav\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.4015 - loss: 1.3912 - val_accuracy: 0.4718 - val_loss: 1.2059\n",
      "Epoch 2/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.7513 - loss: 0.8895 - val_accuracy: 0.5070 - val_loss: 1.0803\n",
      "Epoch 3/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.9765 - loss: 0.3025 - val_accuracy: 0.4977 - val_loss: 1.1381\n",
      "Epoch 4/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0521 - val_accuracy: 0.4812 - val_loss: 1.1626\n",
      "Epoch 5/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0186 - val_accuracy: 0.4789 - val_loss: 1.2052\n",
      "Epoch 6/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0109 - val_accuracy: 0.4648 - val_loss: 1.2492\n",
      "Epoch 7/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 0.4624 - val_loss: 1.2817\n",
      "Epoch 8/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.4624 - val_loss: 1.3073\n",
      "Epoch 9/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.4507 - val_loss: 1.3356\n",
      "Epoch 10/10\n",
      "\u001b[1m69/69\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.4484 - val_loss: 1.3553\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 130  # each word will be presented as a 128-dimensional vector\n",
    "num_classes = 5      # number of target classes\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Input: integer token sequences from your vectorizer\n",
    "    layers.Embedding(\n",
    "        input_dim=MAX_TOKENS, \n",
    "        output_dim=embedding_dim, \n",
    "        mask_zero=True,           # mask padding zeros\n",
    "        input_length=SEQUENCE_LENGTH\n",
    "    ),\n",
    "\n",
    "    # ğŸ”¹ Simple Recurrent Layer (Vanilla RNN)\n",
    "    layers.SimpleRNN(64, return_sequences=False), # outputs only the final hidden state\n",
    "\n",
    "    # Output layer â€” softmax for 5 classes\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",  # integer labels\n",
    "    optimizer=\"adam\", # adaptive learning rate optimizer\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_test_seq, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d2228-a1c5-4cb9-b6ee-c14418870319",
   "metadata": {},
   "source": [
    "# 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c519130-8f27-4f6a-8a0e-5b95d7c51961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RNN Model Final Evaluation ---\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4484 - loss: 1.3553\n",
      "Loss: 1.3553\n",
      "Accuracy: 0.4484\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- RNN Model Final Evaluation ---\")\n",
    "loss, acc = model.evaluate(X_test_seq, y_test, verbose=1)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fca9b-3bf7-451f-9ade-77997d1a2f9a",
   "metadata": {},
   "source": [
    "Pretty bad accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968652bf-f5dc-4ea1-8813-961fdba30884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
